from pathlib import Path
from typing import Optional
from fastapi import FastAPI, Request  # type: ignore
from fastapi.responses import Response, HTMLResponse, RedirectResponse  # type: ignore
from fastapi.templating import Jinja2Templates  # type: ignore
import duckdb  # type: ignore
import pandas as pd  # type: ignore
import matplotlib.pyplot as plt  # type: ignore
import io
import json
from datetime import datetime
import yaml  # type: ignore

# í•œê¸€ í°íŠ¸ ì„¤ì • (ëª¨ë“ˆ import ì‹œ 1íšŒ ì‹¤í–‰)
from src.utils.mpl_korean import setup_korean_font
setup_korean_font()
# ê¸°ì¡´ íŒŒì„œì™€ ìƒˆ íŒŒì„œ ì„ íƒ ê°€ëŠ¥
try:
    from src.nl_parse_v2 import parse_question  # ìƒˆ ë„ë©”ì¸ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ íŒŒì„œ
except ImportError:
    from src.nl_parse import parse_question  # ê¸°ì¡´ íŒŒì„œ (fallback)

# ì •ê·œí™” í•¨ìˆ˜ import
from domain.rules.normalization import normalize
from src.sql_builder import build_sql
from src.process_metrics import (
    build_stable_avg_sql,
    build_overshoot_sql,
    build_dwell_time_sql,
    build_outlier_detection_sql,
    build_trace_compare_sql,
)
<<<<<<< HEAD
from src.chart_templates import get_chart_template, apply_chart_template
from src.payload_builder import build_payload
from src.plot_generator import plot_timeseries
from src.question_suggestions import get_suggestions, get_category_suggestions, get_popular_questions

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent
DB = PROJECT_ROOT / "data_out" / "ald.duckdb"

app = FastAPI(title="ALD NLâ†’SQL Stats API")
templates = Jinja2Templates(directory=str(PROJECT_ROOT / "templates"))

def validate_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ë¬´ê²°ì„± ê²€ì¦: trace_idê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬"""
    if not DB.exists():
        raise FileNotFoundError(f"ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤: {DB}\ní•´ê²°ì±…: python -m src.preprocess_duckdb ì‹¤í–‰ í•„ìš”")
    
    con = duckdb.connect(str(DB))
    try:
        null_count = con.execute("""
            SELECT COUNT(*) 
            FROM traces_dedup 
            WHERE trace_id IS NULL OR trace_id = ''
        """).fetchone()[0]
        
        if null_count > 0:
            total = con.execute("SELECT COUNT(*) FROM traces_dedup").fetchone()[0]
            raise ValueError(
                f"ë°ì´í„° ë¬´ê²°ì„± ì˜¤ë¥˜: trace_idê°€ ë¹„ì–´ìˆëŠ” í–‰ì´ {null_count:,}ê°œ ({null_count/total*100:.1f}%) ìˆìŠµë‹ˆë‹¤.\n"
                f"í•´ê²°ì±…: python -m src.preprocess_duckdb ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¬ìƒì„±í•˜ì„¸ìš”."
            )
    finally:
        con.close()

# ì•± ì‹œì‘ ì‹œ ê²€ì¦
@app.on_event("startup")
async def startup_event():
    try:
        validate_database()
    except (FileNotFoundError, ValueError) as e:
        print(f"âš ï¸  ê²½ê³ : {e}")
        # ì—ëŸ¬ë¥¼ ì¶œë ¥í•˜ì§€ë§Œ ì•±ì€ ê³„ì† ì‹¤í–‰ (ê°œë°œ í¸ì˜ë¥¼ ìœ„í•´)

class QueryIn(BaseModel):
    question: str
=======
from src.charts.renderer import render_chart
from src.services.summary import make_summary
from src.utils.parsed import to_parsed_dict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
DB = PROJECT_ROOT / "data_out" / "ald.duckdb"
SCHEMA_PATH = PROJECT_ROOT / "domain" / "schema" / "columns.yaml"

app = FastAPI(title="ALD NLâ†’SQL Stats API")
templates = Jinja2Templates(directory=str(PROJECT_ROOT / "templates"))
>>>>>>> 378f42a2115c8718668a2287e9ab54018ecf432a

@app.get("/")
def root():
    return RedirectResponse(url="/view")

def load_schema(path: Path) -> dict:
    """columns.yaml ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)"""
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

SCHEMA = load_schema(SCHEMA_PATH)
COLUMNS_SCHEMA = SCHEMA.get("columns", {})
DEFAULTS = SCHEMA.get("defaults", {})
DECIMALS_BY_TYPE = (DEFAULTS.get("decimals_by_type") or {})
UNIT_LABEL = (DEFAULTS.get("unit_label") or {})

def get_format_spec(col_key: Optional[str]) -> tuple[int, str]:
    """
    col_key(canonical key: pressact, mfcmon_n2_1 ë“±) -> (decimals, unit_label) ë°˜í™˜
    ê·œì¹™:
      1) physical_typeë³„ defaults.decimals_by_type ì ìš©
      2) ì»¬ëŸ¼ì— decimalsê°€ ìˆìœ¼ë©´ override
      3) unitì€ defaults.unit_labelë¡œ í™”ë©´ ë¼ë²¨ ë³€í™˜
    """
    if not col_key:
        return (2, "")
    meta = COLUMNS_SCHEMA.get(col_key) or {}
    physical_type = meta.get("physical_type")
    unit_code = meta.get("unit")

    decimals = int(DECIMALS_BY_TYPE.get(physical_type, 2))
    if "decimals" in meta and meta["decimals"] is not None:
        decimals = int(meta["decimals"])

    unit_label = UNIT_LABEL.get(unit_code, unit_code or "")
    return (decimals, unit_label)

def format_value(value: float, col: Optional[str] = None, agg: str = "avg") -> str:
    """ê°’ í¬ë§·íŒ… (ë°˜ì˜¬ë¦¼ + ë‹¨ìœ„)"""
    if value is None or (isinstance(value, float) and (value != value)):  # NaN ì²´í¬
        return "N/A"
    
    # null_ratioëŠ” í¼ì„¼íŠ¸
    if agg == "null_ratio":
        return f"{value:.2f}%"

    decimals, unit_label = get_format_spec(col)
    formatted = f"{value:.{decimals}f}"
    return f"{formatted}{' ' + unit_label if unit_label else ''}"

def format_row(row: dict, parsed: dict) -> dict:
    """í–‰ ë°ì´í„° í¬ë§·íŒ… (n, std ë“± ì¶”ê°€ ì •ë³´ í¬í•¨)"""
    formatted = {}
    col = parsed.get("col") or "pressact"  # ê¸°ë³¸ê°’
    
    for key, value in row.items():
        if key == "value" and col:
            formatted[key] = format_value(value, col, parsed.get("agg", "avg"))
        elif key in ("std", "min_val", "max_val", "avg_diff", "min_diff", "max_diff") and col:
            formatted[key] = format_value(value, col, "avg")
        elif key in ("diff", "diff_signed"):
            # ë¹„êµ ì°¨ì´ëŠ” ì›ë³¸ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ í¬ë§·íŒ…
            formatted[key] = format_value(value, col, "avg") if col else f"{value:.2f}"
        elif key == "n" or key == "outlier_count":
            formatted[key] = int(value) if value else 0
        elif key in ("trace1_avg", "trace2_avg") and col:
            formatted[key] = format_value(value, col, "avg")
        else:
            formatted[key] = value
    
    return formatted

<<<<<<< HEAD
def make_summary(parsed: dict, rows: list) -> str:
    """
    ê²°ê³¼ ìš”ì•½ ìƒì„± (í•´ì„ ë ˆì´ì–´ ì‚¬ìš©)
    
    íŠ¹ìˆ˜ ì¼€ì´ìŠ¤(trace ë¹„êµ, ì´ìƒì¹˜ ë“±)ëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€,
    ì¼ë°˜ ì¼€ì´ìŠ¤ëŠ” interpreter ì‚¬ìš©í•˜ì—¬ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
    """
    from src.interpreter import interpret
    from src.nl_parse import Parsed
    
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: trace ë¹„êµ
    if parsed.get("is_trace_compare") and rows:
        top = rows[0]
        trace_ids = parsed.get("trace_ids", [])
        if len(trace_ids) >= 2:
            step_name = top.get('step_name', '')
            diff_val = top.get('diff', 0)
            trace1_avg = top.get('trace1_avg', 0)
            trace2_avg = top.get('trace2_avg', 0)
            
            diff_str = f"{diff_val:.1f}" if isinstance(diff_val, (int, float)) else str(diff_val)
            trace1_str = f"{trace_ids[0]}"
            trace2_str = f"{trace_ids[1]}"
            
            interpretation = ""
            if step_name == "STANDBY":
                interpretation = "ì´ëŠ” ëŒ€ê¸° ë‹¨ê³„ì—ì„œ ì§„ê³µ ì•ˆì •í™” ë˜ëŠ” ë°°ê¸° ì œì–´ ì°¨ì´ê°€ ìˆì—ˆì„ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤."
            elif step_name in ["B.FILL", "B.FILL4", "B.FILL5"]:
                interpretation = "ì´ëŠ” ì¶©ì§„ ë‹¨ê³„ì—ì„œ ì••ë ¥ ì œì–´ í”„ë¡œíŒŒì¼ ì°¨ì´ê°€ ìˆì—ˆì„ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤."
            elif step_name in ["B.UP", "B.DOWN"]:
                interpretation = "ì´ëŠ” ì••ë ¥ ë³€í™” ë‹¨ê³„ì—ì„œ ì œì–´ ì†ë„ ë˜ëŠ” ëª©í‘œê°’ ì°¨ì´ê°€ ìˆì—ˆì„ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤."
            else:
                interpretation = "ì´ëŠ” í•´ë‹¹ ë‹¨ê³„ì—ì„œ ê³µì • ì¡°ê±´ ë˜ëŠ” ì œì–´ íŒŒë¼ë¯¸í„° ì°¨ì´ê°€ ìˆì—ˆì„ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤."
            
            return f"{step_name} ë‹¨ê³„ì—ì„œ trace ê°„ pressact ì°¨ì´ê°€ ê°€ì¥ í½ë‹ˆë‹¤ (ì°¨ì´: â‰ˆ{diff_str} mTorr, {trace1_str}: {trace1_avg:.1f} mTorr, {trace2_str}: {trace2_avg:.1f} mTorr). {interpretation}"
    
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: ì´ìƒì¹˜ íƒì§€
    if parsed.get("is_outlier"):
        if not rows:
            return "ì´ìƒì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (z-score > 1.0 ê¸°ì¤€)"
        top = rows[0]
        return f"ì´ìƒì¹˜ ë¹„ìœ¨ Top {len(rows)}. 1ìœ„ trace={top.get('trace_id')}: {top.get('value')}% (í‘œë³¸ {top.get('n')}ê°œ, ì´ìƒì¹˜ {top.get('outlier_count')}ê°œ)"
    
    # ì¼ë°˜ ì¼€ì´ìŠ¤: í•´ì„ ë ˆì´ì–´ ì‚¬ìš©
    if not rows:
        from src.interpreter import LABEL, AGG_LABEL
        name = LABEL.get(parsed.get("col"), parsed.get("col")) if parsed.get("col") else "ê°’"
        agg_kor = AGG_LABEL.get(parsed.get("agg"), parsed.get("agg", "ê²°ê³¼"))
        return f"{name} {agg_kor} ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # rowsë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    try:
        df = pd.DataFrame(rows)
        # Parsed ê°ì²´ ìƒì„± (dictì—ì„œ)
        parsed_obj = Parsed(
            agg=parsed.get("agg", "avg"),
            col=parsed.get("col"),
            trace_id=parsed.get("trace_id"),
            trace_ids=parsed.get("trace_ids", []),
            step_name=parsed.get("step_name"),
            step_names=parsed.get("step_names", []),
            group_by=parsed.get("group_by"),
            limit=parsed.get("limit"),
            order=parsed.get("order"),
            date_start=parsed.get("date_start"),
            date_end=parsed.get("date_end"),
            chart_type=parsed.get("chart_type"),
            analysis_type=parsed.get("analysis_type", "ranking"),
            is_stable_avg=parsed.get("is_stable_avg", False),
            is_overshoot=parsed.get("is_overshoot", False),
            is_dwell_time=parsed.get("is_dwell_time", False),
            is_variability=parsed.get("is_variability", False),
            is_outlier=parsed.get("is_outlier", False),
            is_trace_compare=parsed.get("is_trace_compare", False),
        )
        return interpret(parsed_obj, df, topn=5)
    except Exception as e:
        # í´ë°±: ê¸°ì¡´ ë°©ì‹ (ë””ë²„ê¹…ìš©)
        agg_kr_map = {
            "avg": "í‰ê· ", "min": "ìµœì†Œ", "max": "ìµœëŒ€", "count": "ê°œìˆ˜",
            "std": "í‘œì¤€í¸ì°¨", "stddev": "í‘œì¤€í¸ì°¨"
        }
        agg_kr = agg_kr_map.get(parsed.get("agg"), parsed.get("agg", ""))
        col = parsed.get("col") or "*"
        r0 = rows[0] if rows else {}
        if "value" in r0:
            return f"{col} {agg_kr}={r0.get('value')} (í‘œë³¸ {r0.get('n', 0)}ê°œ)"
        return f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

@app.post("/query")
def query(q: QueryIn):
    """í‘œì¤€ payload ë°˜í™˜: question, summary, sql, columns, data, meta"""
    try:
        con = duckdb.connect(str(DB))
        try:
            payload = build_payload(q.question, con)
            return payload
        finally:
            con.close()
    except Exception as e:
        return {
            "ok": False,
            "error": str(e),
            "hint_examples": [
                "standard_trace_001 pressact í‰ê· ",
                "standard_trace_001 ìŠ¤í…ë³„ pressact í‰ê· ",
                "ê³µì •ë³„ pressact í‰ê·  top5",
                "standard_trace_001 step=STANDBY pressact ìµœëŒ€",
                "pressact í‘œì¤€í¸ì°¨",
                "pressact ì¤‘ì•™ê°’",
            ],
        }
=======
def choose_sql(parsed_obj):
    """SQL ë¹Œë” ì„ íƒ (ìš°ì„ ìˆœìœ„: trace_compare > overshoot > outlier > dwell_time > stable_avg > ê¸°ë³¸)"""
    if parsed_obj.is_trace_compare:
        return build_trace_compare_sql(parsed_obj)
    if parsed_obj.is_overshoot:
        return build_overshoot_sql(parsed_obj)
    if parsed_obj.is_outlier:
        return build_outlier_detection_sql(parsed_obj)
    if parsed_obj.is_dwell_time:
        return build_dwell_time_sql(parsed_obj)
    if parsed_obj.is_stable_avg:
        return build_stable_avg_sql(parsed_obj)
    return build_sql(parsed_obj)

def run_query(parsed_obj):
    """SQL ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    sql, params = choose_sql(parsed_obj)
    with duckdb.connect(str(DB), read_only=True) as con:
        df = con.execute(sql, params).df()
    return sql.strip(), params, df

def strip_trailing_limit(sql: str) -> str:
    """ë§¨ ë LIMIT në§Œ ì œê±° (ìœ„í—˜ ìµœì†Œí™”)"""
    import re
    return re.sub(r"\s+LIMIT\s+\d+\s*;?\s*$", "", sql, flags=re.IGNORECASE)

def add_others_row(df_top: pd.DataFrame, df_all: pd.DataFrame) -> pd.DataFrame:
    """Others í–‰ ì¶”ê°€ (Top N ì™¸ ë‚˜ë¨¸ì§€ ë°ì´í„° ìš”ì•½)"""
    if df_all is None or len(df_all) <= len(df_top):
        return df_top

    x_col = df_top.columns[0]
    y_col = "value" if "value" in df_top.columns else ("n" if "n" in df_top.columns else df_top.columns[-1])

    others_df = df_all.iloc[len(df_top):]
    if others_df.empty:
        return df_top

    others_row = {x_col: "Others (ê¸°íƒ€)", y_col: float(others_df[y_col].mean())}

    if "n" in df_all.columns:
        others_row["n"] = int(others_df["n"].sum())
    if "std" in df_all.columns:
        others_row["std"] = float(others_df["std"].mean())
    if "min_val" in df_all.columns:
        others_row["min_val"] = float(others_df["min_val"].min())
    if "max_val" in df_all.columns:
        others_row["max_val"] = float(others_df["max_val"].max())

    return pd.concat([df_top, pd.DataFrame([others_row])], ignore_index=True)
>>>>>>> 378f42a2115c8718668a2287e9ab54018ecf432a

# âœ… HTML í…Œì´ë¸” UI
@app.get("/view", response_class=HTMLResponse)
def view(request: Request, q: str | None = None, show_all: str | None = None):
    if not q:
        return templates.TemplateResponse("index.html", {"request": request, "q": ""})

    try:
        norm = normalize(q)  # âœ… ì¶”ê°€: /queryì™€ ë™ì¼í•˜ê²Œ ì •ê·œí™” ê°ì²´ ìƒì„±
        parsed_obj = parse_question(q)
        
        # ìŠ¤í…ë³„ ì¿¼ë¦¬ëŠ” ê¸°ë³¸ê°’ limit=10 ì ìš© (ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì§€ ì•Šì€ ê²½ìš°)
        # show_all=1ì´ë©´ ì „ì²´ ë³´ê¸°
        if parsed_obj.group_by == "step_name" and parsed_obj.limit is None:
            if show_all != "1":
                parsed_obj.limit = 10
                parsed_obj.order = "desc"
                show_all_button = True
                add_others = True  # Others ê·¸ë£¹ ì¶”ê°€
            else:
                show_all_button = False
                add_others = False
        else:
            show_all_button = False
            add_others = False
        
        # SQL ì‹¤í–‰
        sql, params, df_top = run_query(parsed_obj)
        
<<<<<<< HEAD
        con = duckdb.connect(str(DB))
        try:
            df = con.execute(sql, params).df()
            
            # Others ê·¸ë£¹ ì¶”ê°€ (ìŠ¤í…ë³„ì´ê³  limitì´ ìˆì„ ë•Œ)
            if add_others and parsed_obj.group_by == "step_name" and parsed_obj.limit:
                # y_col ë¨¼ì € ì°¾ê¸°
                y_col_temp = "value" if "value" in df.columns else ("n" if "n" in df.columns else df.columns[-1])
                x_col_temp = df.columns[0]
                
                # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (LIMIT ì œê±°)
                sql_all = sql
                if "LIMIT" in sql_all.upper():
                    # LIMIT ì ˆ ì œê±°
                    import re
                    sql_all = re.sub(r'\s+LIMIT\s+\d+', '', sql_all, flags=re.IGNORECASE)
                
=======
        # Others ê·¸ë£¹ ì¶”ê°€ (ìŠ¤í…ë³„ì´ê³  top_nì´ ìˆì„ ë•Œ)
        df = df_top
        if add_others and parsed_obj.group_by == "step_name" and parsed_obj.top_n:
            # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (LIMIT ì œê±°)
            sql_all = strip_trailing_limit(sql)
            with duckdb.connect(str(DB), read_only=True) as con:
>>>>>>> 378f42a2115c8718668a2287e9ab54018ecf432a
                df_all = con.execute(sql_all, params).df()
            df = add_others_row(df_top, df_all)
        
        rows_raw = df.to_dict(orient="records")
        # í¬ë§·íŒ… ì ìš©
        parsed = to_parsed_dict(parsed_obj)
        rows = [format_row(row, parsed) for row in rows_raw]
        summary = make_summary(parsed, rows_raw)
        return templates.TemplateResponse(
            "index.html",
            {
                "request": request, 
                "q": q,  # ì›ë¬¸ ì§ˆë¬¸
                "question_raw": norm.raw,  # ì›ë¬¸ ì§ˆë¬¸ (ëª…ì‹œì )
                "question_normalized": norm.text,  # ì •ê·œí™”ëœ ì§ˆë¬¸
                "parsed": parsed, 
                "sql": sql.strip(), 
                "rows": rows, 
                "rows_raw": rows_raw,  # ì›ë³¸ ë°ì´í„° (í•„í„°ë§/ì •ë ¬ìš©)
                "summary": summary,
                "show_all_button": show_all_button,
            },
        )
    except Exception as e:
        return templates.TemplateResponse(
            "index.html", 
            {
                "request": request, 
                "q": q,
                "question_raw": norm.raw,
                "question_normalized": norm.text,
                "error": str(e)
            }
        )

# âœ… PNG plot (ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì—´ë¦¬ëŠ” ì—”ë“œí¬ì¸íŠ¸) - ë ˆê±°ì‹œ (í•˜ìœ„ í˜¸í™˜ì„±)
@app.get("/plot")
<<<<<<< HEAD
def plot_legacy(q: str):
    try:
        parsed_obj = parse_question(q)
        
        # ê³µì • ì¹œí™” ì§€í‘œ ë˜ëŠ” ì´ìƒì¹˜ íƒì§€ ì²˜ë¦¬
        if parsed_obj.is_trace_compare:
            sql, params = build_trace_compare_sql(parsed_obj)
            # ë¹„êµëŠ” ë°” ì°¨íŠ¸
            parsed_obj.chart_type = "bar"
        elif parsed_obj.is_overshoot:
            sql, params = build_overshoot_sql(parsed_obj)
            # overshootì€ ìŠ¤í…ë³„ì´ë¯€ë¡œ ê°€ë¡œ ë§‰ëŒ€
            parsed_obj.chart_type = "bar"
        elif parsed_obj.is_outlier:
            sql, params = build_outlier_detection_sql(parsed_obj)
            # ì´ìƒì¹˜ëŠ” traceë³„ì´ë¯€ë¡œ ë°” ì°¨íŠ¸
            parsed_obj.chart_type = "bar"
        elif parsed_obj.is_dwell_time:
            sql, params = build_dwell_time_sql(parsed_obj)
            parsed_obj.chart_type = "bar"
        elif parsed_obj.is_stable_avg:
            sql, params = build_stable_avg_sql(parsed_obj)
        else:
            sql, params = build_sql(parsed_obj)
=======
def plot(q: str):
    parsed_obj = parse_question(q)
    
    # ì°¨íŠ¸ íƒ€ì… ì„¤ì •
    if parsed_obj.is_trace_compare or parsed_obj.is_overshoot or parsed_obj.is_outlier or parsed_obj.is_dwell_time:
        parsed_obj.chart_type = "bar"
    
    # SQL ì‹¤í–‰ ë° ì°¨íŠ¸ ë Œë”ë§
    sql, params, df = run_query(parsed_obj)
    return render_chart(df, parsed_obj)
>>>>>>> 378f42a2115c8718668a2287e9ab54018ecf432a


<<<<<<< HEAD
        if df.empty:
            return Response(content=b"No data", media_type="text/plain")

        # ë‹¨ì¼ ê°’ì´ë©´ ê°„ë‹¨ í…ìŠ¤íŠ¸ë¡œ
        if len(df.columns) == 1 and df.columns[0] in ("value", "n"):
            txt = df.to_string(index=False)
            return Response(content=txt.encode("utf-8"), media_type="text/plain; charset=utf-8")

        # ê·¸ë£¹ ê²°ê³¼: xì¶•(ì²«ë²ˆì§¸ ì»¬ëŸ¼), yì¶•(value or n)
        x_col = df.columns[0]
        y_col = "value" if "value" in df.columns else ("n" if "n" in df.columns else df.columns[-1])

        # ğŸ”¥ í•µì‹¬ ë³€ê²½: ë¶„ì„ ìœ í˜• ê¸°ë°˜ ì°¨íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
        config = get_chart_template(parsed_obj.analysis_type)
        
        # ğŸ”¥ Rule 1: step ê°œìˆ˜ > 12ë©´ â†’ ìš”ì•½ ê·¸ë˜í”„ (group_profileì´ì§€ë§Œ stepì´ ë§ì„ ë•Œ)
        add_others_for_chart = False
        df_all_for_others = None
        if parsed_obj.analysis_type == "group_profile" and parsed_obj.group_by == "step_name" and len(df) > 12:
            # ìš”ì•½ ëª¨ë“œë¡œ ì „í™˜: Top 7 + Others
            if not parsed_obj.limit:  # ì‚¬ìš©ìê°€ ëª…ì‹œí•˜ì§€ ì•Šì•˜ìœ¼ë©´
                config = get_chart_template("ranking")  # ranking ìŠ¤íƒ€ì¼ë¡œ ì „í™˜
                config["chart_type"] = "horizontal_bar"  # ê°€ë¡œ ë§‰ëŒ€ë¡œ
                parsed_obj.limit = 7  # Top 7ë§Œ í‘œì‹œ
                parsed_obj.order = "desc"
                add_others_for_chart = True  # Others ì¶”ê°€ í”Œë˜ê·¸
                # ë‚˜ë¨¸ì§€ëŠ” Othersë¡œ ë¬¶ê¸° ìœ„í•´ ì›ë³¸ ì €ì¥ (ê°’ ê¸°ì¤€ ì •ë ¬ í•„ìš”)
                df_all_for_others = df.copy()
                # ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
                df = df.sort_values(y_col, ascending=False).head(7) if len(df) > 7 else df.sort_values(y_col, ascending=False)
        
        # í…œí”Œë¦¿ ì„¤ì •ì— ë”°ë¼ ë°ì´í„° ì²˜ë¦¬
        if config["use_top_n"] and parsed_obj.limit and len(df) > parsed_obj.limit:
            df = df.head(parsed_obj.limit)
        elif len(df) > 100:
            df = df.head(100)  # ìµœëŒ€ 100ê°œ
        
        # í•œê¸€ ë ˆì´ë¸” ë§¤í•‘
        agg_kr = {"avg": "í‰ê· ", "min": "ìµœì†Œ", "max": "ìµœëŒ€", "count": "ê°œìˆ˜"}.get(parsed_obj.agg, parsed_obj.agg)
        col_kr = parsed_obj.col or "ì „ì²´"
        x_col_kr = "ê³µì • ID" if x_col == "trace_id" else ("ë‹¨ê³„ëª…" if x_col == "step_name" else ("ì¼ì" if x_col == "date" else ("ì‹œê°„" if x_col == "hour" else x_col)))
        y_col_kr = f"{col_kr} {agg_kr}" if parsed_obj.col else agg_kr

        fig, ax = plt.subplots(figsize=(14, 7))
        fig.patch.set_facecolor('white')
        
        # ğŸ”¥ ë¶„ì„ ìœ í˜• ê¸°ë°˜ ê³ ì • í…œí”Œë¦¿ ì ìš©
        if config["chart_type"] == "line" or (parsed_obj.group_by in ("date", "hour", "day") or parsed_obj.date_start or parsed_obj.date_end):
            x_vals = df[x_col].tolist()
            y_vals = df[y_col].astype(float).tolist()
            
            # ë‚ ì§œ/ì‹œê°„ì´ë©´ ì •ë ¬
            if x_col == "date":
                df = df.sort_values("date")
                x_vals = df[x_col].tolist()
                y_vals = df[y_col].astype(float).tolist()
            
            ax.plot(range(len(x_vals)), y_vals, marker='o', linewidth=2, markersize=6, color='#667eea')
            ax.set_xticks(range(len(x_vals)))
            ax.set_xticklabels([str(x) for x in x_vals], rotation=45, ha='right')
            
            # ìµœëŒ€ê°’ í‘œì‹œ
            if y_vals:
                max_idx = y_vals.index(max(y_vals))
                ax.plot(max_idx, y_vals[max_idx], 'ro', markersize=12)
                ax.annotate(f'ìµœëŒ€: {y_vals[max_idx]:.2f}', 
                           xy=(max_idx, y_vals[max_idx]),
                           xytext=(max_idx, y_vals[max_idx] + (max(y_vals) - min(y_vals)) * 0.1),
                           fontsize=10, fontweight='bold',
                           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),
                           ha='center')
        else:
            # ğŸ”¥ í…œí”Œë¦¿ ì ìš© (analysis_type ê¸°ë°˜)
            # Others ê·¸ë£¹ ì¶”ê°€ (ìš”ì•½ ëª¨ë“œì¼ ë•Œ)
            if add_others_for_chart and df_all_for_others is not None and len(df_all_for_others) > len(df):
                others_df = df_all_for_others.sort_values(y_col, ascending=False).iloc[len(df):]
                others_value_sum = sum(r for r in others_df[y_col].astype(float).tolist())
                others_avg = others_value_sum / len(others_df) if len(others_df) > 0 else 0
                others_row = {x_col: f"Others ({len(others_df)}ê°œ)", y_col: others_avg}
                # DataFrameì— Others í–‰ ì¶”ê°€
                df = pd.concat([df, pd.DataFrame([others_row])], ignore_index=True)
            
            apply_chart_template(ax, df, x_col, y_col, config, parsed_obj)
        
        # ì¶• ë ˆì´ë¸” ë° ì œëª© ì„¤ì •
        title_lines = []
        filter_parts = []  # ì´ˆê¸°í™” í•„ìˆ˜
        
        if parsed_obj.analysis_type == "comparison" and "trace1_avg" in df.columns:
            # ë¹„êµ ì°¨íŠ¸ëŠ” ë³„ë„ ì²˜ë¦¬
            ax.set_xlabel("ë‹¨ê³„ëª…", fontsize=12, fontweight='bold', labelpad=10)
            ax.set_ylabel(f"{col_kr} í‰ê·  (mTorr)", fontsize=12, fontweight='bold', labelpad=10)
            title_lines = [f"{col_kr} í‰ê·  ë¹„êµ (ë‹¨ê³„ëª…ë³„)"]
            if parsed_obj.trace_ids and len(parsed_obj.trace_ids) >= 2:
                title_lines.append(f"ê³µì •: {parsed_obj.trace_ids[0]}, {parsed_obj.trace_ids[1]}")
        else:
            # ì¼ë°˜ ì°¨íŠ¸
            ax.set_xlabel(x_col_kr, fontsize=12, fontweight='bold', labelpad=10)
            ax.set_ylabel(y_col_kr, fontsize=12, fontweight='bold', labelpad=10)
            
            # ì œëª© ìƒì„±
            title_lines.append(f"{y_col_kr} ({x_col_kr}ë³„)")
            
            if parsed_obj.trace_id:
                filter_parts.append(f"ê³µì •: {parsed_obj.trace_id}")
            if len(parsed_obj.trace_ids) > 1 and parsed_obj.analysis_type != "comparison":
                filter_parts.append(f"ê³µì •: {', '.join(parsed_obj.trace_ids)}")
            if parsed_obj.step_name:
                filter_parts.append(f"ë‹¨ê³„: {parsed_obj.step_name}")
            if len(parsed_obj.step_names) > 1:
                filter_parts.append(f"ë‹¨ê³„: {', '.join(parsed_obj.step_names)}")
            if parsed_obj.date_start:
                filter_parts.append(f"ì‹œì‘: {parsed_obj.date_start}")
            if parsed_obj.date_end:
                filter_parts.append(f"ì¢…ë£Œ: {parsed_obj.date_end}")
        
        if filter_parts:
            title_lines.append(" | ".join(filter_parts))
        
        title_text = "\n".join(title_lines)
        ax.set_title(title_text, fontsize=13, fontweight='bold', pad=15, loc='center', wrap=True)
        
        ax.grid(True, alpha=0.3, axis='y', linestyle='--')
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        
        plt.tight_layout(pad=3.0)

        buf = io.BytesIO()
        plt.savefig(buf, format="png", dpi=150, bbox_inches='tight', facecolor='white', pad_inches=0.3)
        plt.close(fig)
        buf.seek(0)
        return Response(content=buf.read(), media_type="image/png")
    except Exception as e:
        # ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë°˜í™˜
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.text(0.5, 0.5, f'Error: {str(e)}', 
                ha='center', va='center', fontsize=14, color='red',
                transform=ax.transAxes)
        ax.axis('off')
        buf = io.BytesIO()
        plt.savefig(buf, format="png", dpi=150, bbox_inches='tight')
        plt.close(fig)
        buf.seek(0)
        return Response(content=buf.read(), media_type="image/png")

# âœ… plotì„ í˜ì´ì§€ë¡œ ë³´ê¸°(ì´ë¯¸ì§€ íƒœê·¸ë¡œ ë Œë”ë§)
@app.get("/plot_page", response_class=HTMLResponse)
def plot_page(request: Request, q: str):
    return templates.TemplateResponse("plot.html", {"request": request, "q": q})

# âœ… ë°ì´í„° íƒìƒ‰: ì»¬ëŸ¼ ëª©ë¡
@app.get("/api/columns")
def get_columns():
    con = duckdb.connect(str(DB))
    df = con.execute("DESCRIBE traces").df()
    # slugifyëœ ì»¬ëŸ¼ëª…ë§Œ (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë“¤)
    cols = [row[0] for row in df.values if not row[0].startswith('_') and row[0] != 'No.']
    return {"columns": cols}

# âœ… ë°ì´í„° íƒìƒ‰: ê³µì • ID ëª©ë¡
@app.get("/api/traces")
def get_traces():
    con = duckdb.connect(str(DB))
    df = con.execute("SELECT DISTINCT trace_id FROM traces_dedup ORDER BY trace_id").df()
    return {"traces": df['trace_id'].tolist()}

# âœ… ë°ì´í„° íƒìƒ‰: ë‹¨ê³„ëª… ëª©ë¡
@app.get("/api/steps")
def get_steps():
    con = duckdb.connect(str(DB))
    df = con.execute("SELECT DISTINCT step_name FROM traces_dedup ORDER BY step_name").df()
    return {"steps": df['step_name'].tolist()}

# âœ… CSV ë‹¤ìš´ë¡œë“œ
@app.get("/api/csv")
def download_csv(q: str):
    try:
        parsed_obj = parse_question(q)
        
        if parsed_obj.is_trace_compare:
            sql, params = build_trace_compare_sql(parsed_obj)
        elif parsed_obj.is_overshoot:
            sql, params = build_overshoot_sql(parsed_obj)
        elif parsed_obj.is_outlier:
            sql, params = build_outlier_detection_sql(parsed_obj)
        elif parsed_obj.is_dwell_time:
            sql, params = build_dwell_time_sql(parsed_obj)
        elif parsed_obj.is_stable_avg:
            sql, params = build_stable_avg_sql(parsed_obj)
        else:
            sql, params = build_sql(parsed_obj)
        
        con = duckdb.connect(str(DB))
        try:
            df = con.execute(sql, params).df()
        finally:
            con.close()
        
        csv_content = df.to_csv(index=False)
        
        return Response(
            content=csv_content.encode("utf-8-sig"),  # BOM í¬í•¨ (Excel í˜¸í™˜)
            media_type="text/csv; charset=utf-8",
            headers={"Content-Disposition": f'attachment; filename="query_result_{q[:20]}.csv"'}
        )
    except Exception as e:
        return Response(content=f"ì˜¤ë¥˜: {str(e)}".encode("utf-8"), media_type="text/plain")

# âœ… ë°ì´í„° íƒìƒ‰: ë°ì´í„° ë²”ìœ„
@app.get("/api/range")
def get_data_range():
    con = duckdb.connect(str(DB))
    min_date = con.execute("SELECT MIN(DATE(timestamp)) as min_date FROM traces_dedup").fetchone()[0]
    max_date = con.execute("SELECT MAX(DATE(timestamp)) as max_date FROM traces_dedup").fetchone()[0]
    total_rows = con.execute("SELECT COUNT(*) as cnt FROM traces_dedup").fetchone()[0]
    return {
        "min_date": str(min_date) if min_date else None,
        "max_date": str(max_date) if max_date else None,
        "total_rows": total_rows
    }

@app.get("/api/query")
def query_get(q: str):
    """GET ë°©ì‹: í‘œì¤€ payload ë°˜í™˜: question, summary, sql, columns, data, meta"""
    try:
        con = duckdb.connect(str(DB))
        try:
            payload = build_payload(q, con)
            return payload
        finally:
            con.close()
    except Exception as e:
        return {
            "ok": False,
            "error": str(e),
            "hint_examples": get_popular_questions(5),
        }

@app.get("/api/suggestions")
def get_question_suggestions(q: str = "", category: str = None, limit: int = 10):
    """
    ì§ˆë¬¸ ì¶”ì²œ ë° ìë™ì™„ì„±
    
    Args:
        q: ê²€ìƒ‰ì–´ (ë¶€ë¶„ ë§¤ì¹­)
        category: ì¹´í…Œê³ ë¦¬ í•„í„°
        limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    """
    if category:
        questions = get_category_suggestions(category)
        return {
            "suggestions": [{"question": q, "category": category} for q in questions[:limit]]
        }
    
    suggestions = get_suggestions(q, limit)
    return {"suggestions": suggestions}

@app.get("/api/popular")
def get_popular():
    """ì¸ê¸° ì§ˆë¬¸ ëª©ë¡"""
    return {"questions": get_popular_questions(10)}

@app.get("/api/plot")
def plot_api(q: str):
    """ì‹œê³„ì—´ Plot API: Matplotlib PNG ë°˜í™˜"""
    from urllib.parse import unquote
    
    try:
        q_decoded = unquote(q)
        p = parse_question(q_decoded)
        
        # SQL ìƒì„±
        if p.is_trace_compare:
            sql, params = build_trace_compare_sql(p)
        elif p.is_overshoot:
            sql, params = build_overshoot_sql(p)
        elif p.is_outlier:
            sql, params = build_outlier_detection_sql(p)
        elif p.is_dwell_time:
            sql, params = build_dwell_time_sql(p)
        elif p.is_stable_avg:
            sql, params = build_stable_avg_sql(p)
        else:
            sql, params = build_sql(p)
        
        con = duckdb.connect(str(DB))
        try:
            df = con.execute(sql, params).df()
        finally:
            con.close()
        
        # ì‹œê³„ì—´ Plot ìƒì„±
        from src.semantic_resolver import get_metadata_by_physical_column
        metadata = get_metadata_by_physical_column(p.col) if p.col else None
        unit = metadata.get("unit", "") if metadata else ""
        
        title = q_decoded
        x_col = "timestamp" if "timestamp" in df.columns else (df.columns[0] if not df.empty else "timestamp")
        y_col = "value" if "value" in df.columns else (df.columns[-1] if not df.empty else "value")
        
        buf = plot_timeseries(df, title=title, x_col=x_col, y_col=y_col, unit=unit)
        
        return Response(buf.read(), media_type="image/png")
    except Exception as e:
        # ì—ëŸ¬ ì´ë¯¸ì§€ ë°˜í™˜
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.text(0.5, 0.5, f"ì˜¤ë¥˜: {str(e)}", ha='center', va='center', transform=ax.transAxes, color='red')
        buf = io.BytesIO()
        fig.savefig(buf, format="png")
        buf.seek(0)
        plt.close(fig)
        return Response(buf.read(), media_type="image/png")

# âœ… CSV ë‹¤ìš´ë¡œë“œ
@app.get("/api/csv")
def download_csv(q: str):
    try:
        parsed_obj = parse_question(q)
        
        # ê³µì • ì¹œí™” ì§€í‘œ ë˜ëŠ” ì´ìƒì¹˜ íƒì§€ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„: trace_compare > overshoot > outlier > ê¸°íƒ€)
        if parsed_obj.is_trace_compare:
            sql, params = build_trace_compare_sql(parsed_obj)
        elif parsed_obj.is_overshoot:
            sql, params = build_overshoot_sql(parsed_obj)
        elif parsed_obj.is_outlier:
            sql, params = build_outlier_detection_sql(parsed_obj)
        elif parsed_obj.is_dwell_time:
            sql, params = build_dwell_time_sql(parsed_obj)
        elif parsed_obj.is_stable_avg:
            sql, params = build_stable_avg_sql(parsed_obj)
        else:
            sql, params = build_sql(parsed_obj)
        
        con = duckdb.connect(str(DB))
        df = con.execute(sql, params).df()
        
        csv_str = df.to_csv(index=False)
        return Response(content=csv_str, media_type="text/csv", 
                       headers={"Content-Disposition": f"attachment; filename=query_result.csv"})
    except Exception as e:
        return {"ok": False, "error": str(e)}

# âœ… íˆìŠ¤í† ë¦¬ ì €ì¥/ì¡°íšŒ (ê°„ë‹¨í•œ JSON íŒŒì¼ ê¸°ë°˜)
HISTORY_FILE = PROJECT_ROOT / "data" / "history.json"

@app.post("/api/history")
def save_history(q: QueryIn):
    try:
        if not HISTORY_FILE.parent.exists():
            HISTORY_FILE.parent.mkdir(parents=True)
        
        history = []
        if HISTORY_FILE.exists():
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                history = json.load(f)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœê·¼ ê²ƒë§Œ ìœ ì§€ (ìµœëŒ€ 100ê°œ)
        history = [h for h in history if h['question'] != q.question]
        history.insert(0, {"question": q.question, "timestamp": datetime.now().isoformat()})
        history = history[:100]
        
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        
        return {"ok": True}
    except Exception as e:
        return {"ok": False, "error": str(e)}

@app.get("/api/history")
def get_history():
    try:
        if not HISTORY_FILE.exists():
            return {"history": []}
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
        return {"history": history[:20]}  # ìµœê·¼ 20ê°œë§Œ
    except Exception as e:
        return {"ok": False, "error": str(e)}

# âœ… ì¦ê²¨ì°¾ê¸° ì €ì¥/ì¡°íšŒ
FAVORITES_FILE = PROJECT_ROOT / "data" / "favorites.json"

@app.post("/api/favorites")
def save_favorite(q: QueryIn):
    try:
        if not FAVORITES_FILE.parent.exists():
            FAVORITES_FILE.parent.mkdir(parents=True)
        
        favorites = []
        if FAVORITES_FILE.exists():
            with open(FAVORITES_FILE, 'r', encoding='utf-8') as f:
                favorites = json.load(f)
        
        if q.question not in favorites:
            favorites.append(q.question)
        
        with open(FAVORITES_FILE, 'w', encoding='utf-8') as f:
            json.dump(favorites, f, ensure_ascii=False, indent=2)
        
        return {"ok": True}
    except Exception as e:
        return {"ok": False, "error": str(e)}

@app.get("/api/favorites")
def get_favorites():
    try:
        if not FAVORITES_FILE.exists():
            return {"favorites": []}
        with open(FAVORITES_FILE, 'r', encoding='utf-8') as f:
            favorites = json.load(f)
        return {"favorites": favorites}
    except Exception as e:
        return {"ok": False, "error": str(e)}

@app.delete("/api/favorites")
def delete_favorite(q: QueryIn):
    try:
        if not FAVORITES_FILE.exists():
            return {"ok": True}
        
        with open(FAVORITES_FILE, 'r', encoding='utf-8') as f:
            favorites = json.load(f)
        
        favorites = [f for f in favorites if f != q.question]
        
        with open(FAVORITES_FILE, 'w', encoding='utf-8') as f:
            json.dump(favorites, f, ensure_ascii=False, indent=2)
        
        return {"ok": True}
    except Exception as e:
        return {"ok": False, "error": str(e)}
=======
>>>>>>> 378f42a2115c8718668a2287e9ab54018ecf432a
